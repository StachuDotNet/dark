// Model
// type BotResponseItem =
//   | Text of String
//   | Code of String

let log (s: String): Unit =
  let _ = WASM.Editor.callJSFunction "console.log" [s]
  ()

type ChatHistoryItem =
  // | BotResponse of List<BotResponseItem>
  // | UserPrompt of String
  // TODO: use above
  { author: String; isCode: Bool; text: String }

type Model =
  { systemPrompt: String
    chatHistory: List<ChatHistoryItem> }

// Update
type Msg =
  // TODO: use this
  //| UserGavePrompt of String
  { typ: String // UserGavePrompt
    data: String }


let update (model: Model) (msg: Msg) : Model =

  match msg.typ with
  | "UserGavePrompt" ->
    // I guess, until we have cmds or something,
    // we have to deal with http calls and such in-line, like here
    //let botResponseItems =
    let apiResponse = "beep boop" // OpenAI.getCompletion prompt

      // for now, just return one big text item
      // TODO: later, extract out text and code sections
      //[ BotResponseItem.Text apiResponse ]

    Model
      { systemPrompt = model.systemPrompt
        chatHistory =
          List.append
            model.chatHistory
            [
              ChatHistoryItem { author = "User"; isCode = false; text = msg.Data }
              ChatHistoryItem { author = "Bot"; isCode = false; text = apiResponse }
            ] }

/// Single point of communicating to JS host
///
/// Let the JS host know that the state has been updated,
/// so that it can update the UI accordingly.
let updateStateInJS (newState: Model): Result<Unit, String> =
  match Json.serialize<Model> newState with
  | Ok serialized ->
    let _ = WASM.Editor.callJSFunction "window.stateUpdated" [serialized]
    Ok ()
  | Error err ->
    let _ = WASM.Editor.callJSFunction "console.warn" ["Couldn't serialize - " ++ err]
    Error "Couldn't serialize updated state"


/// Single point of communication from JS host
///
/// Listen for events from the JS host, and update the state accordingly.
let handleEvent (evt: String): Result<String, String> =
  match Json.parse<Msg> evt with
  | Ok msg ->
    match WASM.Editor.getState<Model> with
    | Ok currentState ->
      let newState = update currentState msg

      // returns result, but let's assume it worked...
      let _ = WASM.Editor.setState<Model> newState

      updateStateInJS newState

    | Error err -> Error "Couldn't get current state"
  | Error err -> Error "Couldn't parse raw msg"


// Init
// (things to run on startup, before accepting any events
//  the initial state is set to the result of the final expr)
let systemPrompt =
  WASM.HttpClient.request "get" "http://dark-editor.dlio.localhost:11003/system-prompt" [] Bytes.empty

1+2 // useless -- just to prove we can

let initState =
  match systemPrompt with
  | Ok response ->
    Model
      { systemPrompt = String.fromBytes response.body
        chatHistory =
          [
            ChatHistoryItem { author = "Bot"; isCode = false; text = "Welcome to the Chat!" }
            ChatHistoryItem { author = "User"; isCode = false; text = "Hello, give me code!" }
            ChatHistoryItem { author = "Bot"; isCode = false; text = "Here's some code:" }
            ChatHistoryItem { author = "Bot"; isCode = true; text = "(let a = 1 \n a)" }
          ] }
  | Error err ->
    log "oh no, couldn't fetch system prompt"
    Model
      { systemPrompt = String.fromBytes "nope"
        chatHistory = [ ] }

updateStateInJS initState

initState









// // Models to interop with JS host
// // (TODO: wrap this in `module JS = `)
// type BotResponseJS

// type ChatHistoryItemJS =
//   | BotResponse of { author : string; body : String  }

// type ModelForJS =
//   // should be 1:1 with Model, but nice and serializeable
//   { systemPrompt : string
//     state : string
//     code: String
//     chatHistory: List<ChatHistoryItemJS> }

// // TODO: result
// let stateForJS (state: Model) : ModelForJS =
//   { systemPrompt = state.systemPrompt
//     state = match state.State with
//             | WaitingForFirstInput -> "WaitingForFirstInput"
//             | WaitingForUserInput -> "WaitingForUserInput"
//             | BotIsThinking -> "BotIsThinking"
//     chatHistory =
//       state.chatHistory
//       |> List.map (fun item ->
//            { author = match item.author with User -> "User" | Bot -> "Bot"
//              prompt = item.text }) }

// // TODO: result
// let modelFromJS (model: ModelForJS) : Model =
//   { SystemPrompt = Prompt model.systemPrompt
//     State =
//       match model.state with
//       | "WaitingForFirstInput" -> WaitingForFirstInput
//       | "WaitingForUserInput" -> WaitingForUserInput
//       | "BotIsThinking" -> BotIsThinking
//     ChatHistory =
//       model.chatHistory
//       |> List.map (fun item ->
//            { Author = match item.author with "User" -> User | "Bot" -> Bot
//              Prompt = item.text }) }


// type EventJS = { eventName: String; data: String }

// let msgOfJS (msg: EventJS) : Msg =
//   match msg.eventName with
//   | "userGavePrompt" -> UserGavePrompt (Prompt msg.data)
//   | "botResponded" -> BotResponded (Prompt msg.data)
//   | _ -> failwith "Couldn't parse event name"

// let msgToJS (msg: Msg) : EventJS =
//   match msg with
//   | UserGavePrompt prompt ->
//     { eventName = "userGavePrompt"
//       data = match prompt with Prompt p -> p }
//   | BotResponded prompt ->
//     { eventName = "botResponded"
//       data = match prompt with Prompt p -> p }





// TODO
// type OpenAICompletionRequest = {
//   model: String
//   max_tokens: Int
//   temperature: Float
//   prompt : String
// }

// [<HttpHandler("POST", "/api/handle-user-prompt")>]
// let _handler _req =
//   let openAIRequest =
//     OpenAICompletionRequest
//       { model = "text-davinci-003"
//         max_tokens = 700
//         temperature = 0.7
//         prompt = String.fromBytes request.body }

//   match Json.serialize<OpenAICompletionRequest> openAIRequest with
//   | Ok reqBody ->
//     let headers =
//       [
//         ("authorization", "Bearer " ++ OPENAI_API_KEY)
//         ("content-type", "application/json")
//       ]

//     let openAIResponse =
//       HttpClient.request "POST" "https://api.openai.com/v1/completions" headers (String.toBytes reqBody)

//     match openAIResponse with
//     | Ok r -> Http.response (r.body) 200
//     | Error e -> Http.response (String.toBytes "OpenAI API request failed") 500

//   | Error _e -> Http.response (String.toBytes "Couldn't serialize request") 400